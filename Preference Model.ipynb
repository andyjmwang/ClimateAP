{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pricing Uncertainty Induced by Climate Change\n",
    "by Professor [Michael Barnett](https://sites.google.com/site/michaelduglasbarnett/home), Professor [William Brock](https://www.ssc.wisc.edu/~wbrock/) and Professor [Lars Peter Hansen](https://larspeterhansen.org/), you could find the latest draft [here](https://larspeterhansen.org/research/papers/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is to provide the source code and explanations on how we solve our climate problem under __preference damage setting__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work: Importing required packages, importing parameters setting, defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from supportfunctions import *\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving HJB for Preference Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Smart guesses\n",
    "guess = pickle.load(open('./data/WeightedAverseguess.pickle', \"rb\", -1))\n",
    "v0_guess = guess['v0']\n",
    "q_guess = guess['q']\n",
    "e_guess = guess['e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we outline the procedure used in the code to solve this HJB. We write out the steps here, and then refer to those steps in the code below, more details could be found in the remarks part\n",
    "\n",
    "To solve the nonlinear partial differential equations that characterize the HJB equations for the planner's problems from our model, we use a so-called implicit, finite-difference scheme and a conjugate gradient method. Consultations with Joseph Huang, Paymon Khorrami and Fabrice Tourre played an important role in the software implementation. We briefly outline the steps to this numerical solution method below.\n",
    "\n",
    "Recall that the HJB  equation  of interest for the consumption damages model  includes both minimization and maximization:\n",
    "\\begin{align*} 0 = \\max_{a \\in {\\mathbb A}} \\min_{q > 0, \\int q P(d\\theta) =1 } \\min_{g \\in {\\mathbb R}^m} & - \\delta V(x)  + \\delta (1 - \\kappa) \\left[ \\log \\left( {\\alpha}  - i - j \\right)\n",
    "+ k -  d   \\right] + \\delta \\kappa \\left( \\log e +  r \\right) \\cr &\n",
    "+ {\\frac {\\partial V}{\\partial x}} (x) \\cdot \\left[\\int_\\Theta  \\mu_X(x,a \\mid \\theta) q(\\theta) P(d\\theta)  + \\sigma_X(x) g\\right] \\cr &\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr &\n",
    "+ {\\frac {\\xi_m} 2} g'g + \\xi_p \\int_\\Theta [\\log q(\\theta)]  q(\\theta) P(d \\theta).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed recursively as follows:\n",
    "\n",
    "\n",
    "1) start with a value function guess ${\\widetilde V}(x)$ and a decision function ${\\widetilde a}(x)$;\n",
    "\n",
    "2) given $({\\widetilde V}, {\\widetilde a})$, solve the minimization problem embedded in the HJB equation and produce an exponentially-tilted  density ${\\widehat  q}$ and drift distortion ${\\widehat g}$  conditioned on $x$ and  using the  approach described   in section D;\n",
    "\n",
    "3) compute the implied relative entropy from the change in prior:\n",
    "$$\n",
    "{\\widehat {\\mathbb I}}(x) = \\int_\\Theta [\\log {\\widehat q}(\\theta)]  {\\widehat q}(\\theta) P(d \\theta);\n",
    "$$\n",
    "\n",
    "4)  solve the following maximization problem by choice of $a=(i,j,e)$:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\delta (1 - \\kappa)  \\log \\left( {\\alpha}  - i - j \\right)\n",
    " + \\delta \\kappa  \\log e   \\cr &\n",
    "+ {\\frac {\\partial V}{\\partial x}} (x) \\cdot \\int_\\Theta  \\mu_X\\left(x,a   \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta);\n",
    "\\end{align*}\n",
    "\n",
    "   a) Compute ${\\hat i}$ and ${\\hat j}$ by solving the two first-order conditions for $i$ and $j$ with cobweb-style iterations.  Cobweb iterations converge or diverge depending the relative slopes of supply and demand functions.  By shrinking the step size, these slopes can be altered.\n",
    "\n",
    "Expand the two equation system by adding a third equation that defines  a common \"price\" $p$,\n",
    "$$\n",
    "p =  {\\frac {\\delta (1-\\kappa)}  {\\alpha  - i - j} } = g(i+j).\n",
    "$$\n",
    "Write the two first-order conditions as\n",
    "$$\n",
    "p = {\\frac {\\phi_0 \\phi_1 V_k(x) }{ 1 + \\phi_1 i}} = f_1(i)\n",
    "$$\n",
    "$$\n",
    "p  = V_r(x)  \\left(  {\\psi_0 \\psi_1} \\right) j^{\\psi_1 - 1}  \\exp\\left[ \\psi_1(k -  r)\\right]  = f_2(j).\n",
    "$$\n",
    "\n",
    "Given $p$  and for step size $\\tilde{\\epsilon}$, compute\n",
    "\n",
    "* $i^* = (f_1)^{-1}(p)$\n",
    "\n",
    "* $j^* = (f_2)^{-1}(p)$\n",
    "\n",
    "* $p^* = {\\eta} g(i^* + j^*) + \\left(1 - {\\eta} \\right) p$\n",
    "\n",
    "* set $p  = p^*$.\n",
    "\n",
    "Iterate to convergence.\n",
    "\n",
    "\n",
    "b) Compute ${\\hat e}$ by solving the first-order conditions\n",
    "$$\n",
    "{\\frac {\\delta \\kappa} e} +  {\\frac d {d e}} \\left[V_x(x)  \\cdot \\int_\\Theta  \\mu_X\\left(x, i, j, a  \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta)\\right] = 0 .\n",
    "$$\n",
    "These first-order conditions turn out not to depend on $(i,j)$.\n",
    "\n",
    "\n",
    "5) use the minimization output from step (2) and  maximization output from step (4) and construct an adjusted drift using the following formula, which is the analog to formula (20) from the paper:\n",
    "\\begin{equation*}\n",
    "{\\widehat \\mu}(x) = \\int_\\Theta  \\mu_X\\left(x, {\\widehat a}  \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta) + \\sigma_X(x) {\\widehat g}(x);\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6)  construct the linear equation system for a new value function $V = {\\widehat V}$:\n",
    "\\begin{align*}\n",
    "0 =  & - \\delta V(x)  + \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right]\n",
    "+ k -  d   \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] \\cr\n",
    "& + {\\frac {\\partial V}{\\partial x}} (x) \\cdot {\\widehat \\mu}(x)\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr\n",
    "& + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x);\n",
    "\\end{align*}\n",
    "\n",
    "7) modify this equation by adding a so-called \"false transient\" to the left-hand side:\n",
    "\\begin{align} \\label{modify_linear}\n",
    "{\\frac {V(x) - {\\widetilde V}(x)} \\epsilon}  =  & - \\delta V(x)  + \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right]\n",
    "+ k -  d    \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] \\cr\n",
    "& + {\\frac {\\partial V}{\\partial x}} (x) \\cdot {\\widehat \\mu}(x)\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr\n",
    "& + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x);\n",
    "\\end{align}\n",
    "\n",
    "8) solve the linear system from step (7) for $V= {\\widehat V}$ using a conjugate-gradient method;\n",
    "\n",
    "9) set ${\\widetilde V} = {\\widehat V}$ and ${\\widetilde a} = {\\widehat a}$ and repeat steps (2) - (8)  until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobweb Passed, iterations: 1, i error:   0.000000, j error:   0.000000\n",
      "Episode 1: PDE Error: 0.0000062475; False Transient Error: 0.0000000100; Iterations: 2; CG Error: 0.0000000001\n",
      "Episode 2: PDE Error: 0.0000062475; False Transient Error: 0.0000000100; Iterations: 2; CG Error: 0.0000000001\n",
      "--- 24.203909158706665 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Damage function choices\n",
    "damageSpec = 'Weighted'  # Choose among \"High\"(Weitzman), 'Low'(Nordhaus) and 'Weighted'\n",
    "\n",
    "if damageSpec == 'High':\n",
    "    weight = 0.0\n",
    "elif damageSpec == 'Low':\n",
    "    weight = 1.0\n",
    "else:\n",
    "    weight = 0.5\n",
    "\n",
    "Œæ‚Çö =  1 / 4000  # Ambiguity Averse Paramter \n",
    "# Sensible choices are from 0.0002 to 4000, while for parameters input over 0.01 the final results won't alter as much\n",
    "    \n",
    "McD = np.loadtxt('./data/TCRE_MacDougallEtAl2017_update.txt')\n",
    "par_lambda_McD = McD / 1000\n",
    "\n",
    "Œ≤ùòß = np.mean(par_lambda_McD)  # Climate sensitivity parameter, MacDougall (2017)\n",
    "œÉ·µ¶ = np.var(par_lambda_McD, ddof = 1)  # varaiance of climate sensitivity parameters\n",
    "Œª = 1.0 / œÉ·µ¶ \n",
    "\n",
    "# Parameters are defined the same as they are in the paper\n",
    "Œ¥ = 0.01        \n",
    "Œ∫ = 0.032       \n",
    "œÉùò® = 0.02\n",
    "œÉùò¨ = 0.0161\n",
    "œÉùò≥ = 0.0339 \n",
    "Œ± = 0.115000000000000\n",
    "œï0 = 0.0600\n",
    "œï1 = 16.666666666666668\n",
    "ŒºÃÑ‚Çñ = -0.034977443912449\n",
    "œà0 = 0.112733407891680\n",
    "œà1 = 0.142857142857143\n",
    "\n",
    "# parameters for damage function settings\n",
    "power = 2 \n",
    "Œ≥1 = 0.00017675\n",
    "Œ≥2 = 2. * 0.0022\n",
    "Œ≥2_plus = 2. * 0.0197\n",
    "Œ≥ÃÑ2_plus = weight * 0 + (1 - weight) * Œ≥2_plus\n",
    "\n",
    "œÉ1 = 0\n",
    "œÉ2 = 0\n",
    "œÅ12 = 0\n",
    "FÃÑ = 2\n",
    "crit = 2\n",
    "F0 = 1\n",
    "\n",
    "xi_d = -1 * (1 - Œ∫)\n",
    "\n",
    "# Regarding the choice of Œµ and Œ∑ please refers to Remark 1\n",
    "# False Trasient Time step\n",
    "Œµ = 0.1\n",
    "# Cobweb learning rate\n",
    "Œ∑ = 0.05\n",
    "\n",
    "\n",
    "# Specifying Tolerance level\n",
    "tol = 1e-8\n",
    "\n",
    "# Grids Specification\n",
    "\n",
    "# Coarse Grids\n",
    "# nR = 30\n",
    "# nF = 40\n",
    "# nK = 25\n",
    "# R = np.linspace(R_min, R_max, nR)\n",
    "# F = np.linspace(F_min, F_max, nF)\n",
    "# K = np.linspace(K_min, K_max, nK)\n",
    "\n",
    "# hR = R[1] - R[0]\n",
    "# hF = F[1] - F[0]\n",
    "# hK = K[1] - K[0]\n",
    "\n",
    "# Dense Grids\n",
    "\n",
    "R_min = 0\n",
    "R_max = 9\n",
    "F_min = 0\n",
    "F_max = 4000\n",
    "K_min = 0\n",
    "K_max = 18\n",
    "\n",
    "hR = 0.05\n",
    "hF = 25\n",
    "hK = 0.15\n",
    "\n",
    "R = np.arange(R_min, R_max + hR, hR)\n",
    "nR = len(R)\n",
    "F = np.arange(F_min, F_max + hF, hF)\n",
    "nF = len(F)\n",
    "K = np.arange(K_min, K_max + hK, hK)\n",
    "nK = len(K)\n",
    "\n",
    "# Here's how we discretize our state space formulating PDE, see Remark 2\n",
    "(R_mat, F_mat, K_mat) = np.meshgrid(R,F,K, indexing = 'ij')\n",
    "stateSpace = np.hstack([R_mat.reshape(-1,1,order = 'F'),F_mat.reshape(-1,1,order = 'F'),K_mat.reshape(-1,1,order = 'F')])\n",
    "\n",
    "# Inputs for function quad_int; Integrating across parameter distribution\n",
    "quadrature = 'legendre'\n",
    "n = 30\n",
    "a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)\n",
    "b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)\n",
    "\n",
    "v0 = Œ∫ * R_mat + (1-Œ∫) * K_mat - Œ≤ùòß * F_mat\n",
    "\n",
    "episode = 1\n",
    "FC_Err = 1\n",
    "v0 = v0_guess\n",
    "q = q_guess\n",
    "e_star = e_guess\n",
    "\n",
    "while episode == 0 or FC_Err > tol:\n",
    "    vold = v0.copy()\n",
    "    # Applying finite difference scheme to the value function\n",
    "    v0_dr = finiteDiff(v0,0,1,hR) \n",
    "    v0_df = finiteDiff(v0,1,1,hF)\n",
    "    v0_dk = finiteDiff(v0,2,1,hK)\n",
    "\n",
    "    v0_drr = finiteDiff(v0,0,2,hR)\n",
    "    v0_drr[v0_dr < 1e-16] = 0\n",
    "    v0_dr[v0_dr < 1e-16] = 1e-16\n",
    "    v0_dff = finiteDiff(v0,1,2,hF)\n",
    "    v0_dkk = finiteDiff(v0,2,2,hK)\n",
    "\n",
    "    if episode == 0:\n",
    "        # First time into the loop\n",
    "        B1 = v0_dr - xi_d * (Œ≥1 + Œ≥2 * F_mat * Œ≤ùòß + Œ≥2_plus * (F_mat * Œ≤ùòß - FÃÑ) ** (power - 1) * (F_mat >= (crit / Œ≤ùòß))) * Œ≤ùòß * np.exp(R_mat) - v0_df * np.exp(R_mat)\n",
    "        C1 = - Œ¥ * Œ∫\n",
    "        e = -C1 / B1\n",
    "        e_hat = e\n",
    "        Acoeff = np.ones(R_mat.shape)\n",
    "        Bcoeff = ((Œ¥ * (1 - Œ∫) * œï1 + œï0 * œï1 * v0_dk) * Œ¥ * (1 - Œ∫) / (v0_dr * œà0 * 0.5) * np.exp(0.5 * (R_mat - K_mat))) / (Œ¥ * (1 - Œ∫) * œï1)\n",
    "        Ccoeff = -Œ±  - 1 / œï1\n",
    "        j = ((-Bcoeff + np.sqrt(Bcoeff ** 2 - 4 * Acoeff * Ccoeff)) / (2 * Acoeff)) ** 2\n",
    "        i = Œ± - j - (Œ¥ * (1 - Œ∫)) / (v0_dr * œà0 * 0.5) * j ** 0.5 * np.exp(0.5 * (R_mat - K_mat))\n",
    "        q = Œ¥ * (1 - Œ∫) / (Œ± - i - j)\n",
    "    else:\n",
    "        e_hat = e_star\n",
    "        \n",
    "        # Step 4 (a) : Cobeweb scheme to update i and j; q is an intermediary variable that determines i and j\n",
    "        Converged = 0\n",
    "        nums = 0\n",
    "        while Converged == 0:\n",
    "            i_star = (œï0 * œï1 * v0_dk / q - 1) / œï1\n",
    "            j_star = (q * np.exp(œà1 * (R_mat - K_mat)) / (v0_dr * œà0 * œà1)) ** (1 / (œà1 - 1))\n",
    "            if Œ± > np.max(i_star + j_star):\n",
    "                q_star = Œ∑ * Œ¥ * (1 - Œ∫) / (Œ± - i_star - j_star) + (1 - Œ∑) * q\n",
    "            else:\n",
    "                q_star = 2 * q\n",
    "            if np.max(abs(q - q_star) / Œ∑) <= 1e-5:\n",
    "                Converged = 1\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            else:\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            \n",
    "            nums += 1\n",
    "        print('Cobweb Passed, iterations: {}, i error: {:10f}, j error: {:10f}'.format(nums, np.max(i - i_star), np.max(j - j_star)))\n",
    "\n",
    "    a1 = np.zeros(R_mat.shape)\n",
    "    b1 = xi_d * e_hat * np.exp(R_mat) * Œ≥1\n",
    "    c1 = 2 * xi_d * e_hat * np.exp(R_mat) * F_mat * Œ≥2 \n",
    "    ŒªÃÉ1 = Œª + c1 / Œæ‚Çö\n",
    "    Œ≤ÃÉ1 = Œ≤ùòß - c1 * Œ≤ùòß / (Œæ‚Çö * ŒªÃÉ1) -  b1 /  (Œæ‚Çö * ŒªÃÉ1)\n",
    "    I1 = a1 - 0.5 * np.log(Œª) * Œæ‚Çö + 0.5 * np.log(ŒªÃÉ1) * Œæ‚Çö + 0.5 * Œª * Œ≤ùòß ** 2 * Œæ‚Çö - 0.5 * ŒªÃÉ1 * (Œ≤ÃÉ1) ** 2 * Œæ‚Çö\n",
    "    #     R1 = \\xi\\_p.*(I1-(a1+b1.*Œ≤ÃÉ1+c1./2.*(Œ≤ÃÉ1).^2+c1./2./\\lambda\\tilde_1));\n",
    "    R1 = 1 / Œæ‚Çö * (I1 - (a1 + b1 * Œ≤ÃÉ1 + c1 / 2 * Œ≤ÃÉ1 ** 2 + c1 / 2 / ŒªÃÉ1))\n",
    "    J1_without_e = xi_d * (Œ≥1 * Œ≤ÃÉ1 + Œ≥2 * F_mat * (Œ≤ÃÉ1 ** 2 + 1 / ŒªÃÉ1)) * np.exp(R_mat)\n",
    "\n",
    "    œÄÃÉ1 = weight * np.exp(-1 / Œæ‚Çö * I1)\n",
    "\n",
    "    # Step (2), solve minimization problem in HJB and calculate drift distortion, see remark 3 for more details\n",
    "    def scale_2_fnc(x):\n",
    "        return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat)  * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "    scale_2 = quad_int(scale_2_fnc, a, b, n, 'legendre')\n",
    "\n",
    "    def q2_tilde_fnc(x):\n",
    "        return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat) / scale_2\n",
    "\n",
    "    I2 = -1 * Œæ‚Çö * np.log(scale_2)\n",
    "\n",
    "    def J2_without_e_fnc(x):\n",
    "        return xi_d * np.exp(R_mat) * q2_tilde_fnc(x) * (Œ≥1 * x + Œ≥2 * F_mat * x ** 2 + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "    J2_without_e = quad_int(J2_without_e_fnc, a, b, n, 'legendre')\n",
    "    J2_with_e = J2_without_e * e_hat\n",
    "\n",
    "    R2 = (I2 - J2_with_e) / Œæ‚Çö\n",
    "    œÄÃÉ2 = (1 - weight) * np.exp(-1 / Œæ‚Çö * I2)\n",
    "    œÄÃÉ1_norm = œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2)\n",
    "    œÄÃÉ2_norm = 1 - œÄÃÉ1_norm\n",
    "\n",
    "    # step 4 (b) updating e based on first order conditions\n",
    "    expec_e_sum = (œÄÃÉ1_norm * J1_without_e + œÄÃÉ2_norm * J2_without_e)\n",
    "\n",
    "    B1 = v0_dr - v0_df * np.exp(R_mat) - expec_e_sum\n",
    "    C1 = -Œ¥ * Œ∫\n",
    "    e = -C1 / B1\n",
    "    e_star = e\n",
    "\n",
    "    J1 = J1_without_e * e_star\n",
    "    J2 = J2_without_e * e_star\n",
    "\n",
    "    # Step (3) calculating implied entropies\n",
    "    I_term = -1 * Œæ‚Çö * np.log(œÄÃÉ1 + œÄÃÉ2)\n",
    "\n",
    "    R1 = (I1 - J1) / Œæ‚Çö\n",
    "    R2 = (I2 - J2) / Œæ‚Çö\n",
    "    \n",
    "    # Step (5) solving for adjusted drift\n",
    "    drift_distort = (œÄÃÉ1_norm * J1 + œÄÃÉ2_norm * J2)\n",
    "\n",
    "    if weight == 0 or weight == 1:\n",
    "        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2\n",
    "    else:\n",
    "        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2 + œÄÃÉ1_norm * np.log(\n",
    "            œÄÃÉ1_norm / weight) + œÄÃÉ2_norm * np.log(œÄÃÉ2_norm / (1 - weight))\n",
    "\n",
    "    RE_total = Œæ‚Çö * RE\n",
    "\n",
    "    # Step (6) and (7) Formulating HJB False Transient ODE parameters, See remark 4 for more details\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    B_r = -e_star + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_f = e_star * np.exp(R_mat)\n",
    "    B_k = ŒºÃÑ‚Çñ + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    D = Œ¥ * Œ∫ * np.log(e_star) + Œ¥ * Œ∫ * R_mat + Œ¥ * (1 - Œ∫) * (np.log(Œ± - i - j) + K_mat) + drift_distort + RE_total # + I_term \n",
    "\n",
    "    # Step (8) solving linear system using a conjugate-gradient method in C++, see remark 5, 6 for more details\n",
    "    out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, 'False Transient')\n",
    "\n",
    "    out_comp = out[2].reshape(v0.shape,order = \"F\")\n",
    "    \n",
    "    # Calculating PDE Error and False Transient (lhs) Error\n",
    "    PDE_rhs = A * v0 + B_r * v0_dr + B_f * v0_df + B_k * v0_dk + C_rr * v0_drr + C_kk * v0_dkk + C_ff * v0_dff + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    FC_Err = np.max(abs((out_comp - v0)))\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "    episode += 1\n",
    "    \n",
    "    # step 9: keep iterating until convergence\n",
    "    v0 = out_comp\n",
    "\n",
    "print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 1\n",
    "The choices of ${\\eta}$ in step (4) and $\\epsilon$ in step (7) are made by trading off increases in speed of convergence, achieved by increasing their magnitudes, and enhancing stability of the iterative algorithm, achieved by decreasing their magnitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 2\n",
    "We discretize the state space of $x$ using a set number of points along each of the three dimensions and impose a  fixed step size between points for each of these dimensions.  For interior points, we approximate the first derivatives using a first-order upwind scheme while the second derivatives are calculated using a central difference scheme. Upwind schemes are one-sided difference approximations that use the sign of the drifts for the states to determine the direction of the difference.  (See, for instance,  \"An Introduction to Finite Difference Methods for PDE methods in Finance\" by Agnes Tourin, Fields Institute.)  At boundary points we sometimes only have one option used in the approximation. We use a symmetric second difference approximation whenever possible and switch to a one-sided approximation as needed at boundary points.  With this construction, we have reduced the right-hand side of equaton in step (7) as a matrix applied to the value function at the chosen set of discrete points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 3\n",
    "\n",
    "The term of interest is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "(\\kappa -1) \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_3\\beta\\left(\\beta f - {\\overline \\gamma} \\right)^2 \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \\exp(r) e\n",
    "\\end{eqnarray}\n",
    "\n",
    "Take ${\\widehat e}$ from the previous iteration.\n",
    "\n",
    "\n",
    "##### Low damage model\n",
    "\n",
    "Since  $\\gamma_3 = 0$ for the low damage model, use quasi analytical approach.\n",
    "\n",
    "We wish to compute:\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal I}_1  = - \\xi_a \\log E\\left( \\exp\\left[ - {\\frac 1 {\\xi_a}} \\left(  {\\sf a}_1 + {\\sf b}_1 \\beta  + {\\frac {{\\sf c}_1} 2}   \\beta ^2 \\right) \\right] \\mid i \\right)  .\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{align*}\n",
    "{\\sf a}_1& = 0,  \\cr\n",
    " {\\sf b}_1   & =  (\\kappa - 1) {\\widehat e} \\exp(r)  \\gamma_1, \\cr\n",
    "{\\sf c}_1   & =  (\\kappa -1) {\\widehat e} \\exp(r)  f \\gamma_2.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We apply a well known __complete-the-sqaures__ exploiting the exponential quadratic form of the normal density function.\n",
    "Specifically, we compute the worst-case precision by equating the log- quadratic terms in $\\beta$\n",
    "\\begin{eqnarray}\n",
    " \\lambda  + {\\frac {{\\sf c}_1}{\\xi_a}} = {\\widetilde \\lambda}_1\n",
    "\\end{eqnarray}\n",
    "where $\\lambda$ is the baseline precision and ${\\widetilde \\lambda}_i$ is the worst-case precision.\n",
    "\n",
    "For calculating the worst-case mean, we equate first order terms in $\\beta$\n",
    "\\begin{eqnarray}\n",
    "{\\overline \\beta} \\lambda - {\\frac 1 {\\xi_a}}  {\\sf b}_1 = {\\widetilde \\beta}_1 {\\widetilde \\lambda}_1\n",
    "\\end{eqnarray}\n",
    "Thus the worst-case mean is:\n",
    "\\begin{eqnarray}\n",
    "{\\widetilde \\beta}_1   = {\\overline \\beta}\\left({\\frac {\\lambda}{{\\widetilde \\lambda}_1}} \\right)   - {\\frac 1 {\\xi_a {{\\widetilde \\lambda}_1 }}} {\\sf b}_1  = {\\overline \\beta} -  {\\frac {{\\sf c}_1}{\\xi_a \\widetilde \\lambda_i}}{\\overline \\beta}\n",
    "-  {\\frac 1 {\\xi_a {{\\widetilde \\lambda}_1 }}} {\\sf b}_1\n",
    "\\end{eqnarray}\n",
    "Finally, we compute ${\\mathcal I}_i$ by a) bringing it inside an exponential term,  b) multiplying this by the worst-case normal density that we just deduced, and c) equating the constant terms:\n",
    "\\begin{equation*}\n",
    "- {\\frac 1 {\\xi_a}} {\\mathcal I}_1   +   {\\frac 1 2} \\log {\\widetilde \\lambda}_1 - {\\frac 1   2} \\left({\\widetilde  \\beta}_1 \\right)^2 {\\widetilde \\lambda}_1  = - {\\frac 1 {\\xi_a}} {\\sf a}_1 + {\\frac 1  2} \\log \\lambda - {\\frac 1 2} \\left({\\overline \\beta} \\right)^2 \\lambda .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus\n",
    "\\begin{equation*}\n",
    "{\\mathcal I}_1  = {\\sf a}_1  -   {\\frac {\\xi_a}  2} \\log \\lambda + {\\frac {\\xi_a}  2} \\log {\\widetilde \\lambda}_1\n",
    " + {\\frac {\\xi_a \\lambda}  2} \\left({\\overline \\beta} \\right)^2   - {\\frac {\\xi_a {\\widetilde \\lambda}_1}  2} \\left({\\widetilde  \\beta}_1 \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal R}_1  = {\\frac 1 {\\xi_a}} \\left({\\mathcal I}_1 -  \\left[ {\\sf a}_1  + {\\sf b}_1 {\\widetilde \\beta}_1\n",
    "+ {\\sf c}_1  \\left({\\widetilde \\beta}_1 \\right)^2 + {\\frac {{\\sf c}_1} { {\\widetilde \\lambda}_1 }} \\right]  \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal J}_1(e) = (\\kappa -1)  \\left( \\gamma_1 {\\widetilde \\beta}_1 + \\gamma_2 \\left[  \\left({\\widetilde \\beta}_1\\right)^2\n",
    "+ {\\frac 1 {\\widetilde \\lambda}_1}\\right]\n",
    "f\\right)  \\exp(r) e\n",
    "\\end{eqnarray}\n",
    "\n",
    "##### High damage model\n",
    "\n",
    "We must proceed differently for the high damage model with numerical computation.\n",
    "Form\n",
    "\\begin{eqnarray}\n",
    "{\\rm num}_2(\\beta)  = \\exp\\left( - {\\frac 1 \\xi_a} (\\kappa -1) \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_2^+ \\beta\\left(\\beta f - {\\overline \\gamma} \\right) \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \\exp(r) {\\widehat e}\\right),\n",
    "\\end{eqnarray}\n",
    "compute:\n",
    "\\begin{eqnarray}\n",
    "{\\rm scale}_2 = \\int_\\beta {\\rm num}_2(\\beta) p(\\beta) d\\beta\n",
    "\\end{eqnarray}\n",
    "\n",
    "via Gauss-Hermite quadrature, \n",
    "and form a density conditioned on model two:\n",
    "\\begin{eqnarray}\n",
    "{\\widetilde q}_2(\\beta)  = {\\frac {{\\rm num}(\\beta)}{{\\rm scale}2}}\n",
    "\\end{eqnarray}\n",
    "relative to $p(\\beta)$.  \n",
    "\n",
    "\n",
    "Also compute:\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal  I}_2 =  -\\xi_a \\log {\\rm scale}_2,\n",
    "\\end{eqnarray}\n",
    "along with:\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal J}_2(e) = {\\frac {(\\kappa - 1) \\left( \\displaystyle\\int_\\beta    \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_2^+\\beta\\left(\\beta f - {\\overline \\gamma} \\right) \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \n",
    "{\\rm num}_2(\\beta)p(\\beta) d \\beta \\right) \\exp(r) e}{{\\rm scale}_2}}\n",
    "\\end{eqnarray}\n",
    "using Gaussian-Hermite quadrature for the numerator integral. \n",
    "Notice that ${\\mathcal J}_2(e)$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    "\n",
    "With these computations in hand, form relative entropy conditioned on model two:\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal R}_2 = {\\frac 1 {\\xi_a}} \\left[{\\mathcal I}_2 - {\\mathcal J}_2\\left( {\\widehat e} \\right) \\right] .\n",
    "\\end{eqnarray}\n",
    "Notice that ${\\mathcal J}_2(e)$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    "\n",
    "##### Distorted model Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 4\n",
    "Recall that in order to solve the prefernce HJB model, we transform the question in solving a linear system of equations.\n",
    "\\begin{eqnarray}\n",
    "{\\frac {V(x) - {\\widetilde V}(x)} \\epsilon}  & =  &   V(x) \\cdot A(x)+{\\frac {\\partial V}{\\partial x}} (x) \\cdot  B(x)   + {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\cdot C(x)+ D(x) \n",
    "\\end{eqnarray}\n",
    "Where\n",
    "\\begin{eqnarray}\n",
    "A(x) &=& - \\delta \\\\\n",
    "B(x) &=& {\\widehat \\mu}(x)\\\\\n",
    "C(x) &=& {\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' I \\sigma_X(x) \\right] \\\\\n",
    "D(x) &=& \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right] + k -  d  \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Precisely speaking our value function is a three-dimension tensor $ V(R,T,K) $ with default grid size of (181,161,121) in \"F\" order. In order to solve the linear system, we reshape $ V(x) $ into a one dimension array with size of (3526061,1) and conduct similar adjustments to $ A(x) $, $ B(x) $, $ C(x) $ and $ D(x) $ respectively.\n",
    "\n",
    "In \"F\" order, when we call our initial value function at grid point (i,j,k) $ V(i,j,k) $, we instead call equivalent reshaped value fnction $V(i + j * N_i + k * N_i * N_j)$ where $N_i$, $N_j$ and $N_k$ stands for number of grid poitns in that dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 5\n",
    "\n",
    "We solve the matrix counterpart to the equation in step (8) using the conjugate gradient algorithm.  This is a well known iterative algorithm designed to  solve a minimization problem:  $\\frac 1 2 (\\Lambda y - \\lambda)'(\\Lambda y - \\lambda)$ for a  nonsingular matrix $\\Lambda$ and vector $\\lambda$.   The $y$ that minimizes this expression  satisfies the linear equation $\\Lambda y = \\lambda$.  The matrix $\\Lambda$ and vector $\\lambda$ come from the numerical approximation of below equation.  We measure the conjugate gradient error by\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{{\\frac {(\\Lambda y - \\lambda)'(\\Lambda y - \\lambda)}{\\lambda'\\lambda}}}.\n",
    "\\end{equation}\n",
    "\n",
    "We prespecify a conjugate gradient error bound and a bound on the difference in value functions between iterations and take as the starting point for conjugate gradient the output from the previous iteration.  We achieve convergence when the difference in value functions between iterations satisfies a prespecified error bound.  Upon convergence, we compute the maximum error for the matrix approximation to the right-hand side of equation system in step (7). We call this the maximum pde error.\n",
    "\n",
    "#### Remark 6\n",
    "While we are computing one-sided difference approximations at boundary points, we are not imposing additional boundary conditions on our finite state space as is often done when solving pde's with regular boundaries. Instead we aim to approximate pde solutions for the stochastic differential equation with unattainable boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 7\n",
    "\n",
    "In choosing the tolerance level, we tested the time it takes to solve the PDE numerically is decreasing with the size of $\\epsilon$, while the stability of the program is also decreasing with $\\epsilon$. \n",
    "\\begin{eqnarray*}\n",
    "\\max_x \\frac{|V(x)-\\widetilde{V}(x)|}{\\epsilon} < tol\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We choose time varying $\\epsilon$ to balance the efficiency of the program and for room to explore different magnitude of the uncertainty.\n",
    "* We used variable step sizes to speed program up while ensuring convergence: 0.3 for the first 1000 iterations, 0.2 for 1001-2000 iterations and 0.1 going forward.\n",
    "*  For averse low damage case, normal variable step sizes scheme didn't work so we used 0.05 after 7000 iterations. \n",
    "*  We used fixed step size at 0.1 for growth neutral case to ensure convergence\n",
    "\n",
    "Below table listed the convergence criteria and final errors for solving HJBs at different preference setting. We solved all models listed in this table by False Transient Algorithm. All outer loop convergence were evaluted on the change in value functions. \n",
    "\n",
    "| ambiguity    |    damage    | Cobweb tolerance |  outer loop tolerance  | CG tolerance |  PDE Error |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "averse|  high  | 1e-5 | 1e-8 | 1e-10 | 6.88e-6 |\n",
    "averse| weighted | 1e-5 | 1e-8 | 1e-10 | 6.25e-6|\n",
    "averse|low| 1e-5 | 1e-8 | 1e-10 | 3.83e-6|\n",
    "neutral|high  | 1e-5 | 1e-8 | 1e-10 | 5.67e-6|\n",
    "neutral|weighted  | 1e-5 | 1e-8 | 1e-10 | 4.60e-6|\n",
    "neutral|low  | 1e-5 | 1e-8 | 1e-10 | 3.73e-6|\n",
    "\n",
    "Below table listed the convergence criteria and final errors for solving HJBs under growth settings. \n",
    "\n",
    "\n",
    "| ambiguity    |    damage    | Cobweb tolerance |  outer loop tolerance  | CG tolerance |  PDE Error |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "averse | -  | 1e-5 | 1e-8 | 1e-10 | 1.59e-5\n",
    "neutral  | -  | 1e-5 | 1e-8 | 1e-10 | 1.27e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridInterp():\n",
    "\n",
    "    def __init__(self, grids, values, method = 'Linear'):\n",
    "\n",
    "        # unpacking\n",
    "        self.grids = grids\n",
    "        (self.xs, self.ys, self.zs) = grids\n",
    "        self.nx = len(self.xs)\n",
    "        self.ny = len(self.ys)\n",
    "        self.nz = len(self.zs)\n",
    "        \n",
    "        self.values = values\n",
    "\n",
    "        assert (self.nx, self.ny, self.nz) == values.shape, \"ValueError: Dimensions not match\"\n",
    "        self.method = method\n",
    "\n",
    "    def get_value(self, x, y, z):\n",
    "\n",
    "        if self.method == 'Linear':\n",
    "            \n",
    "            func = RegularGridInterpolator(self.grids, self.values)\n",
    "            return func([x,y,z])[0]\n",
    "\n",
    "        elif self.method == 'Spline':\n",
    "\n",
    "            func1 = CubicSpline(self.xs, self.values)\n",
    "            yzSpace = func1(x)\n",
    "            \n",
    "            func2 = CubicSpline(self.ys, yzSpace)\n",
    "            zSpace = func2(y)\n",
    "            \n",
    "            func3 = CubicSpline(self.zs, zSpace)\n",
    "            return func3(z)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Method Not Supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Linear'\n",
    "T = 100\n",
    "pers = 4 * T\n",
    "dt = T / pers\n",
    "nDims = 5\n",
    "its = 1\n",
    "\n",
    "gridpoints = (R, F, K)\n",
    "\n",
    "e_func_r = GridInterp(gridpoints, e, method)\n",
    "def e_func(x):\n",
    "    return e_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "j_func_r = GridInterp(gridpoints, j, method)\n",
    "def j_func(x):\n",
    "    return max(j_func_r.get_value(np.log(x[0]), x[2], np.log(x[1])), 0)\n",
    "\n",
    "i_func_r = GridInterp(gridpoints, i, method)\n",
    "def i_func(x):\n",
    "    return i_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "v_drfunc_r = GridInterp(gridpoints, v0_dr, method)\n",
    "def v_drfunc(x):\n",
    "    return v_drfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "v_dtfunc_r = GridInterp(gridpoints, v0_df, method)\n",
    "def v_dtfunc(x):\n",
    "    return v_dtfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "v_dkfunc_r = GridInterp(gridpoints, v0_dk, method)\n",
    "def v_dkfunc(x):\n",
    "    return v_dkfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "v_func_r = GridInterp(gridpoints, v0, method)\n",
    "def v_func(x):\n",
    "    return v_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "pi_tilde_1_func_r = GridInterp(gridpoints, œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2), method)\n",
    "def pi_tilde_1_func(x):\n",
    "    return pi_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "pi_tilde_2_func_r = GridInterp(gridpoints, œÄÃÉ2 / (œÄÃÉ1 + œÄÃÉ2), method)\n",
    "def pi_tilde_2_func(x):\n",
    "    return pi_tilde_2_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "def scale_2_fnc(x):\n",
    "    return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat)  * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "scale_2 = quad_int(scale_2_fnc, a, b, n, 'legendre')\n",
    "\n",
    "def q2_tilde_fnc(x):\n",
    "    return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat) / scale_2\n",
    "\n",
    "def base_model_drift_func(x):\n",
    "    return np.exp(R_mat) * e * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥ÃÑ2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "base_model_drift =  quad_int(base_model_drift_func, a, b, n, 'legendre')\n",
    "\n",
    "mean_nordhaus = Œ≤ÃÉ1\n",
    "lambda_tilde_nordhaus = ŒªÃÉ1\n",
    "nordhaus_model_drift = (Œ≥1 * mean_nordhaus + Œ≥2 * (1 / lambda_tilde_nordhaus + mean_nordhaus ** 2) * F_mat) * np.exp(R_mat) * e\n",
    "\n",
    "def weitzman_model_drift_func(x):\n",
    "    return np.exp(R_mat) * e * q2_tilde_fnc(x) * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "weitzman_model_drift = quad_int(weitzman_model_drift_func, a, b, n, 'legendre')\n",
    "\n",
    "nordhaus_drift_func_r = GridInterp(gridpoints, nordhaus_model_drift, method)\n",
    "def nordhaus_drift_func(x):\n",
    "    return nordhaus_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "weitzman_drift_func_r = GridInterp(gridpoints, weitzman_model_drift, method)\n",
    "def weitzman_drift_func(x):\n",
    "    return weitzman_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "base_drift_func_r = GridInterp(gridpoints, base_model_drift, method)\n",
    "def base_drift_func (x): \n",
    "    return base_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# function handles\n",
    "def muR(x):\n",
    "    return -e_func(x) + œà0 * (j_func(x) * x[1] / x[0]) ** œà1\n",
    "def muK(x): \n",
    "    return (ŒºÃÑk + œï0 * np.log(1 + i_func(x) * œï1))\n",
    "def muF(x):\n",
    "    return e_func(x) * x[0]\n",
    "def muD_base(x):\n",
    "    return base_drift_func(x)\n",
    "def muD_tilted(x):\n",
    "    return pi_tilde_1_func(x) * nordhaus_drift_func(x) + (1 - pi_tilde_1_func(x)) * weitzman_drift_func(x)\n",
    "\n",
    "def sigmaR(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaK(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaF(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaD(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "\n",
    "# initial points\n",
    "R_0 = 650\n",
    "K_0 = 80 / Œ±\n",
    "F_0 = 870 - 580\n",
    "initial_val = np.array([R_0, K_0, F_0])\n",
    "D_0_base = muD_base(initial_val)\n",
    "D_0_tilted = muD_tilted(initial_val)\n",
    "\n",
    "# Set bounds\n",
    "R_max_sim = np.exp(max(R))\n",
    "K_max_sim = np.exp(max(K))\n",
    "F_max_sim = max(F)\n",
    "D_max_sim = 5.0\n",
    "\n",
    "R_min_sim = np.exp(min(R))\n",
    "K_min_sim = np.exp(min(K))\n",
    "F_min_sim = min(F)\n",
    "D_min_sim = -5\n",
    "\n",
    "upperbounds = np.array([R_max_sim, K_max_sim, F_max_sim, D_max_sim, D_max_sim])\n",
    "lowerbounds = np.array([R_min_sim, K_min_sim, F_min_sim, D_min_sim, D_min_sim])\n",
    "\n",
    "hists = np.zeros([pers, nDims, its])\n",
    "# hists = hists.copy()\n",
    "e_hists = np.zeros([pers,its])\n",
    "# e_hists = e_hists.copy()\n",
    "j_hists = np.zeros([pers,its])\n",
    "# j_hists = j_hists.copy()\n",
    "i_hists = np.zeros([pers,its])\n",
    "# i_hists = i_hists.copy()\n",
    "\n",
    "v_dr_hists = np.zeros([pers,its])\n",
    "v_dt_hists = np.zeros([pers,its])\n",
    "v_dk_hists = np.zeros([pers,its])\n",
    "v_hists = np.zeros([pers,its])\n",
    "\n",
    "for iters in range(0,its):\n",
    "    hist = np.zeros([pers,nDims])\n",
    "    e_hist = np.zeros([pers,1])\n",
    "    i_hist = np.zeros([pers,1])\n",
    "    j_hist = np.zeros([pers,1])\n",
    "\n",
    "    v_dr_hist = np.zeros([pers,1])\n",
    "    v_dt_hist = np.zeros([pers,1])\n",
    "    v_dk_hist = np.zeros([pers,1])\n",
    "    v_hist = np.zeros([pers,1])\n",
    "\n",
    "    hist[0,:] = [R_0, K_0, F_0, D_0_base, D_0_tilted]\n",
    "    e_hist[0] = e_func(hist[0,:]) * hist[0,0]\n",
    "    i_hist[0] = i_func(hist[0,:]) * hist[0,1]\n",
    "    j_hist[0] = j_func(hist[0,:]) * hist[0,0]\n",
    "    v_dr_hist[0] = v_drfunc(hist[0,:])\n",
    "    v_dt_hist[0] = v_dtfunc(hist[0,:])\n",
    "    v_dk_hist[0] = v_dkfunc(hist[0,:])\n",
    "    v_hist[0] = v_func(hist[0,:])\n",
    "\n",
    "    for tm in range(1,pers):\n",
    "        shock = norm.rvs(0,np.sqrt(dt),nDims)\n",
    "        # print(muR(hist[tm-1,:]))\n",
    "        hist[tm,0] = cap(hist[tm-1,0] * np.exp((muR(hist[tm-1,:])- 0.5 * sum((sigmaR(hist[tm-1,:])) ** 2))* dt + sigmaR(hist[tm-1,:]).dot(shock)),lowerbounds[0], upperbounds[0])\n",
    "        hist[tm,1] = cap(hist[tm-1,1] * np.exp((muK(hist[tm-1,:])- 0.5 * sum((sigmaK(hist[tm-1,:])) ** 2))* dt + sigmaK(hist[tm-1,:]).dot(shock)),lowerbounds[1], upperbounds[1])\n",
    "        hist[tm,2] = cap(hist[tm-1,2] + muF(hist[tm-1,:]) * dt + sigmaF(hist[tm-1,:]).dot(shock), lowerbounds[2], upperbounds[2])\n",
    "        hist[tm,3] = cap(hist[tm-1,3] + muD_base(hist[tm-1,:]) * dt + sigmaD(hist[tm-1,:]).dot(shock), lowerbounds[3], upperbounds[3])\n",
    "        hist[tm,4] = cap(hist[tm-1,4] + muD_tilted(hist[tm-1,:]) * dt + sigmaD(hist[tm-1,:]).dot(shock), lowerbounds[4], upperbounds[4])\n",
    "\n",
    "        e_hist[tm] = e_func(hist[tm-1,:]) * hist[tm-1,0]\n",
    "        i_hist[tm] = i_func(hist[tm-1,:]) * hist[tm-1,1]\n",
    "        j_hist[tm] = j_func(hist[tm-1,:]) * hist[tm-1,0]\n",
    "\n",
    "        v_dr_hist[tm] = v_drfunc(hist[tm-1,:])\n",
    "        v_dt_hist[tm] = v_dtfunc(hist[tm-1,:])\n",
    "        v_dk_hist[tm] = v_dkfunc(hist[tm-1,:])\n",
    "        v_hist[tm] = v_func(hist[tm-1,:])\n",
    "\n",
    "    hists[:,:,iters] = hist\n",
    "    e_hists[:,[iters]] = e_hist\n",
    "    i_hists[:,[iters]] = i_hist\n",
    "    j_hists[:,[iters]] = j_hist\n",
    "\n",
    "    v_dr_hists[:,[iters]] = v_dr_hist\n",
    "    v_dt_hists[:,[iters]] = v_dt_hist\n",
    "    v_dk_hists[:,[iters]] = v_dk_hist\n",
    "    v_hists[:,[iters]] = v_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCC Calculation Feyman Kac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading smart guesses\n",
    "base_ = loadmat(r'C:\\Users\\jiamingwang\\Dropbox\\share with John\\Final Code\\Preference\\SCC_solution\\SCC_base_averse_weighted.mat')\n",
    "base_guess = base_['v0']\n",
    "worst_ = loadmat(r'C:\\Users\\jiamingwang\\Dropbox\\share with John\\Final Code\\Preference\\SCC_solution\\SCC_worst_averse_weighted.mat')\n",
    "worst_guess = worst_['v0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As in the body of the paper, consider impulse response functions for\n",
    "the logarithm of damages in the future induced by a marginal change in emissions today.  The responses are  necessarily nonlinear impulse responses and hence will be state-dependent.  The marginal  emissions change induces\n",
    "an  impact on $\\log D_{t+u}$ given by\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\left([\\nabla \\Gamma](\\beta F_t) \\beta  + \\zeta_D(Z_t) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right)\n",
    "+ \\int_0^u [\\nabla^2 \\Gamma](\\beta F_{t+\\tau}) \\beta^2 E_{t+\\tau} d \\tau.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "The first contribution occurs on impact, and the second one accumulates through the effect of current emissions on the state variable $f$.\n",
    "\n",
    "Consider the specification where damages enter the utility function discounted and multiplied by $\\delta (1-\\kappa)$.  Recall that by doing  some simple accounting and exploiting the exponential discounting used for the discounted marginal damage response,   we ecombine all the date $\\tau$ contributions for $u \\ge \\tau$ to obtain\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\exp(-\\delta \\tau)  (1-\\kappa) [\\nabla^2 \\Gamma]( \\beta F_{t+\\tau} ) \\beta^2 E_{t+\\tau},\n",
    "\\end{eqnarray}\n",
    "\n",
    "along with the initial term\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\exp(-\\delta \\tau) \\delta (1-\\kappa) \\left([\\nabla \\Gamma](\\beta F_t) \\beta  + \\zeta_D(Z_t) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right).\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "The external part to the social cost of carbon is the expected exponentially discounted future impulse responses.   In the absence of ambiguity and robustness concerns it is given by\n",
    "\n",
    "$$\\begin{align} \n",
    "& \\delta (1-\\kappa) \\left([\\nabla \\Gamma](\\beta F_t) \\beta  + \\zeta_D(Z_t) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right) \\int_0^\\infty\n",
    "\\exp(-\\delta \\tau) d\\tau  \\cr\n",
    "& + E \\left[ \\int_0^\\infty \\exp(-\\delta \\tau) (1-\\kappa) [\\nabla^2 \\Gamma]( \\beta F_{t+\\tau} ) \\beta^2 E_{t+\\tau} d \\tau \\mid X_t = x \\right]\n",
    "\\tag{1}\n",
    "\\end{align}$$\n",
    "\n",
    "divided by the date $t$ marginal utility of consumption. \n",
    "\n",
    "By integrating the exponential function in the first expression, the $\\delta$ drops out resulting in\n",
    "\n",
    "\\begin{eqnarray}\n",
    "(1-\\kappa) \\left([\\nabla \\Gamma](\\beta F_t) \\beta  + \\zeta_D(Z_t) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "which is one of the two terms in formula $(1)$ for $ecc$.\n",
    "\n",
    "\n",
    "Since the second term is a  discounted expected value, it solves a so-called __Feynman-Kac (FK) equation__.   Formally, we are interested in the solution $\\Phi$ to the forward-looking equation\n",
    "\n",
    "$$\\begin{align} \\label{FKsolution}\n",
    "\\Phi(X_t) & = {\\mathbb E} \\left[ \\int_0^\\infty \\exp( - \\delta \\tau) \\Psi(X_{t+\\tau}) d \\tau \\mid X_t \\right] \\cr\n",
    "& = \\exp(\\delta t)  \\int_t^\\infty \\exp( - \\delta \\tau) {\\mathbb E} \\left[ \\Psi(X_{\\tau}) \\mid X_t \\right] d \\tau\n",
    "\\tag{2}\n",
    "\\end{align}$$\n",
    "\n",
    "for a pre-specified $\\Psi$. Specifically, let\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Psi(x) =   (1-\\kappa) [\\nabla^2 \\Gamma] ( \\beta f ) \\beta^2  e^*(x) \\exp(r).\n",
    "\\end{eqnarray}\n",
    "\n",
    "To provide a heuristic reminder of form and rationale for the FK equation,  we obtain the drift of the process $\\{ \\Phi(X_t) : t \\ge 0 \\}$ of the left-hand hand side of formula $(2)$ via Ito's formula for $X_t = x$ as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\frac {\\partial \\Phi}{\\partial x}} (x) \\cdot \\mu_X[x,a^*(x)]\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 \\Phi}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right],\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $a^*$ is the maximizing decision rule. Differentiating the right-hand side of $(2)$ with respect to $t$ gives an alternative formula for this drift:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\delta \\Phi(x)  - \\Psi(x).\n",
    "\\end{eqnarray}\n",
    "\n",
    "By equating these, we obtain the FK or (more generally) resolvent equation:\n",
    "\n",
    "\\begin{equation} \n",
    "- \\delta \\Phi(x)  + {\\frac {\\partial \\Phi}{\\partial x}} (x) \\cdot \\mu_X[x,a^*(x)]\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 \\Phi}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] + \\Psi(x)=0.\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "By differentiating the HJB equation with respect to $f$ and applying the __Envelope Theorem__, it can be shown that the solution $\\Phi$  to the FK equation satisfies\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Phi(x) = - V_f(x).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The analogous arguments apply in the presence of ambiguity and robustness concerns except that we use the altered probability distribution \n",
    "when computing expectations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the uncertainty aversion we need to address the tilted probability $Q^*$. We solve numerically a corresponding Feynman-Kac equation with the worst-case evolution.  We let\n",
    "\\begin{align*}\n",
    "\\widetilde{\\Psi(x)} =  (1 - \\kappa) \\int_\\Theta \\nabla^2 \\Gamma(\\beta f) \\beta^2  dQ^*\\left(\\theta \\mid x \\right)  {\\hat e \\left(x  \\right) \\exp(r)} .\n",
    "\\end{align*}\n",
    "The corresponding $\\widetilde{\\Phi}$ is to the solution of the below PDE\n",
    "\\begin{align*}\n",
    "-\\delta \\widetilde{\\Phi(x)} + {\\frac {\\partial \\widetilde{\\Phi}}{\\partial x}} (x) \\cdot \\mu_X[x,a^*(x)] +{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 \\widetilde{\\Phi}}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] + \\widetilde{\\Psi(x)}=0.\n",
    "\\end{align*}\n",
    "\n",
    "Note that this PDE is different from the above one in only the last term $\\Psi$. The previous case is a special case for the worst-case evolution, with the uncertainty parameter $\\xi_a=\\infty$.\n",
    "\n",
    "In calculating $\\widetilde{\\Psi}$ we choose \"Gauss-Legendre\" quadrature with 30 integration points on $[\\beta_f-5\\sigma_f,\\beta_f+5\\sigma_f] $ in computing the term that involves high damage models.\n",
    "\n",
    "We again solve the PDE using the conjugate gradient solver using the below form:\n",
    "\\begin{eqnarray}\n",
    "\\widetilde{\\Phi(x)} \\cdot A(x)+{\\frac {\\partial\\widetilde{ \\Phi}}{\\partial x}} (x) \\cdot  B(x)   + trace [S(x)'\\frac {\\partial^2 \\widetilde{\\Phi}}{\\partial x \\partial x'}(x)S(x)] + D(x) & =  &  0\n",
    "\\end{eqnarray}\n",
    "With the following coefficients:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A(x) &=& - \\delta \\\\\n",
    "B(x) &=& \\mu_X[x,a^*(x)]\\\\\n",
    "C(x) &=& {\\frac 1 2} \\sigma_X(x)'I\\sigma_X(x)  \\\\\n",
    "D(x) &=& \\widetilde{\\Psi(x)}\\\\\n",
    "\\end{eqnarray}\n",
    "with $C(x) = S(x)'S(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Œæ‚Çö > 100:  # We consider for Œæ‚Çö level over 100 as \n",
    "    \n",
    "    # Under ambiguity neutrality, we can write explicit form solution for Social Cost of Carbon\n",
    "    MC = Œ¥ * (1-Œ∫) / (Œ± * np.exp(K_mat) - i * np.exp(K_mat) - j * np.exp(R_mat))      # Marginal utility of consumption\n",
    "    ME = Œ¥ * Œ∫ / (e * np.exp(R_mat))      # Marginal utility of emission\n",
    "    SCC = 1000 * ME / MC\n",
    "    SCC_func_r = GridInterp(gridpoints, SCC, method)\n",
    "    \n",
    "    def SCC_func(x): \n",
    "        return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "    \n",
    "    SCC_values = np.zeros([pers,its])\n",
    "    \n",
    "    # Interpolating values from simulated trajectory\n",
    "    for tm in range(pers):\n",
    "        for path in range(its): \n",
    "            SCC_values[tm, path] = SCC_func(hists[tm,:,path])\n",
    "\n",
    "    SCC_total = np.mean(SCC_values,axis = 1)\n",
    "\n",
    "    SCCs['SCC'] = SCC_total\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Base model\n",
    "    def base_model_flow_func(x):\n",
    "        return (Œ≥2 * x ** 2 + Œ≥ÃÑ2_plus * x ** 2 * ((x * F_mat - FÃÑ) >=0)) * np.exp(R_mat) * e *  norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "    base_model_flow = quad_int(base_model_flow_func, a, b, n, 'legendre')\n",
    "    flow_base = base_model_flow\n",
    "\n",
    "    # input for solver\n",
    "\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    ####\n",
    "    B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_k = ŒºÃÑ‚Çñ + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    B_f = e * np.exp(R_mat)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    D = flow_base\n",
    "\n",
    "    out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, base_guess, solverType='Feyman Kac')\n",
    "    v0_base = out[2].reshape(v0.shape, order=\"F\")\n",
    "    v0_base = v0_base\n",
    "\n",
    "    v0_dr_base = finiteDiff(v0_base,0,1,hR) \n",
    "    v0_df_base = finiteDiff(v0_base,1,1,hF)\n",
    "    v0_dk_base = finiteDiff(v0_base,2,1,hK)\n",
    "\n",
    "    v0_drr_base = finiteDiff(v0_base,0,2,hR)\n",
    "    v0_dff_base = finiteDiff(v0_base,1,2,hF)\n",
    "    v0_dkk_base = finiteDiff(v0_base,2,2,hK)\n",
    "\n",
    "    v0_drr_base[v0_dr_base < 1e-16] = 0\n",
    "    v0_dr_base[v0_dr_base < 1e-16] = 1e-16\n",
    "\n",
    "    PDE_rhs = A * v0_base + B_r * v0_dr_base + B_f * v0_df_base + B_k * v0_dk_base + C_rr * v0_drr_base + C_kk * v0_dkk_base + C_ff * v0_dff_base + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    print(\"Feyman Kac Base Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))\n",
    "\n",
    "    # Worst Model\n",
    "    mean_nordhaus = Œ≤ÃÉ1\n",
    "    lambda_tilde_nordhaus = ŒªÃÉ1\n",
    "    def scale_2_fnc(x):\n",
    "        return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e)  * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "    scale_2 = quad_int(scale_2_fnc, a, b, n, 'legendre')\n",
    "\n",
    "    def q2_tilde_fnc(x):\n",
    "        return np.exp(-1 / Œæ‚Çö * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e) / scale_2\n",
    "\n",
    "    nordhaus_model_flow = (Œ≥2 * (1 / lambda_tilde_nordhaus + mean_nordhaus ** 2)) * np.exp(R_mat) * e \n",
    "    # weitzman_model_flow_func = @(x) q2_tilde_1_fnc(x) .*(gamma_2.*x.^2 +gamma_2_plus.*x.^2.*((x.*t_mat-f_bar)>=0)).*exp(r_mat).*e .*normpdf(x,beta_f,sqrt(var_beta_f));\n",
    "    def weitzman_model_flow_func(x): \n",
    "        return q2_tilde_fnc(x) * (Œ≥2 * x ** 2 + Œ≥2_plus * x ** 2 * ((x * F_mat - FÃÑ) >= 0 )) * np.exp(R_mat) * e * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "    weitzman_model_flow = quad_int(weitzman_model_flow_func, a, b, n, 'legendre')\n",
    "\n",
    "    I1 = a1 - 0.5 * np.log(Œª) * Œæ‚Çö + 0.5 * np.log(ŒªÃÉ1) * Œæ‚Çö + 0.5 * Œª * Œ≤ùòß ** 2 * Œæ‚Çö - 0.5 * ŒªÃÉ1 * (Œ≤ÃÉ1) ** 2 * Œæ‚Çö\n",
    "    I2 = -1 * Œæ‚Çö * np.log(scale_2)\n",
    "    œÄÃÉ1 = (weight) * np.exp(-1 / Œæ‚Çö * I1)\n",
    "    œÄÃÉ2 = (1 - weight) * np.exp(-1 / Œæ‚Çö * I2)\n",
    "    œÄÃÉ1_norm = œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2)\n",
    "    œÄÃÉ2_norm = 1 - œÄÃÉ1_norm\n",
    "\n",
    "    flow_tilted = œÄÃÉ1_norm * nordhaus_model_flow + œÄÃÉ2_norm * weitzman_model_flow\n",
    "\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    ####\n",
    "    B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_k = ŒºÃÑ‚Çñ + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    B_f = e * np.exp(R_mat)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    D = flow_tilted\n",
    "\n",
    "    out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, worst_guess, solverType='Feyman Kac')\n",
    "    v0_worst = out[2].reshape(v0.shape, order=\"F\")\n",
    "    v0_worst = v0_worst\n",
    "\n",
    "    v0_dr_worst = finiteDiff(v0_worst,0,1,hR) \n",
    "    v0_df_worst = finiteDiff(v0_worst,1,1,hF)\n",
    "    v0_dk_worst = finiteDiff(v0_worst,2,1,hK)\n",
    "\n",
    "    v0_drr_worst = finiteDiff(v0_worst,0,2,hR)\n",
    "    v0_dff_worst = finiteDiff(v0_worst,1,2,hF)\n",
    "    v0_dkk_worst = finiteDiff(v0_worst,2,2,hK)\n",
    "    v0_drr_worst[v0_dr_worst < 1e-16] = 0\n",
    "    v0_dr_worst[v0_dr_worst < 1e-16] = 1e-16\n",
    "\n",
    "    PDE_rhs = A * v0_worst + B_r * v0_dr_worst + B_f * v0_df_worst + B_k * v0_dk_worst + C_rr * v0_drr_worst + C_kk * v0_dkk_worst + C_ff * v0_dff_worst + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    print(\"Feyman Kac Worst Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))\n",
    "\n",
    "\n",
    "    # SCC decomposition\n",
    "\n",
    "    v0_dr = finiteDiff(v0,0,1,hR) \n",
    "    v0_df = finiteDiff(v0,1,1,hF)\n",
    "    v0_dk = finiteDiff(v0,2,1,hK)\n",
    "\n",
    "    v0_drr = finiteDiff(v0,0,2,hR)\n",
    "    v0_dff = finiteDiff(v0,1,2,hF)\n",
    "    v0_dkk = finiteDiff(v0,2,2,hK)\n",
    "\n",
    "    v0_drr[v0_dr < 1e-16] = 0\n",
    "    v0_dr[v0_dr < 1e-16] = 1e-16\n",
    "\n",
    "    gridpoints = (R, F, K)  # can modify\n",
    "\n",
    "    MC = Œ¥ * (1-Œ∫) / (Œ± * np.exp(K_mat) - i * np.exp(K_mat) - j * np.exp(R_mat))\n",
    "    ME = Œ¥ * Œ∫ / (e * np.exp(R_mat))\n",
    "    SCC = 1000 * ME / MC\n",
    "    SCC_func_r = GridInterp(gridpoints, SCC, method)\n",
    "\n",
    "    def SCC_func(x): \n",
    "        return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    ME1 = v0_dr * np.exp(-R_mat)\n",
    "    SCC1 = 1000 * ME1 / MC\n",
    "    SCC1_func_r = GridInterp(gridpoints, SCC1, method)\n",
    "    def SCC1_func(x):\n",
    "        return SCC1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    ME2_base = (1-Œ∫) * v0_base\n",
    "    SCC2_base = 1000 * ME2_base / MC\n",
    "    SCC2_base_func_r = GridInterp(gridpoints, SCC2_base, method)\n",
    "    def SCC2_base_func(x):\n",
    "        return SCC2_base_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    def V_d_baseline_func(x):\n",
    "        return xi_d * (Œ≥1 * x + Œ≥2 * F_mat * x** 2 +\n",
    "                        Œ≥ÃÑ2_plus * x * (x * F_mat - FÃÑ) * (power - 1)\n",
    "                        * ((x * F_mat - FÃÑ) >= 0 )) * norm.pdf(x, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    V_d_baseline = quad_int(V_d_baseline_func, a, b, n, 'legendre')\n",
    "    ME2b = -V_d_baseline\n",
    "    SCC2_V_d_baseline = 1000 * ME2b / MC\n",
    "    SCC2_V_d_baseline_func_r = GridInterp(gridpoints, SCC2_V_d_baseline, method)\n",
    "    def SCC2_V_d_baseline_func(x):\n",
    "        return SCC2_V_d_baseline_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    ME2_tilt = (1-Œ∫) * v0_worst\n",
    "    SCC2_tilt = 1000 * ME2_tilt / MC\n",
    "    SCC2_tilt_func_r = GridInterp(gridpoints, SCC2_tilt, method)\n",
    "    def SCC2_tilt_func(x):\n",
    "        return SCC2_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "\n",
    "    ME2b = -expec_e_sum * np.exp(-R_mat)\n",
    "    SCC2_V_d_tilt_ = 1000 * ME2b / MC\n",
    "    SCC2_V_d_tilt_func_r = GridInterp(gridpoints, SCC2_V_d_tilt_, method)\n",
    "    def SCC2_V_d_tilt_func(x):\n",
    "        return SCC2_V_d_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "\n",
    "    SCC_values = np.zeros([pers,its])\n",
    "    SCC1_values = np.zeros([pers,its])\n",
    "    SCC2_base_values = np.zeros([pers,its])\n",
    "    SCC2_tilt_values = np.zeros([pers,its])\n",
    "    SCC2_V_d_baseline_values = np.zeros([pers,its])\n",
    "    SCC2_V_d_tilt_values = np.zeros([pers,its])\n",
    "\n",
    "    for tm in range(pers):\n",
    "        for path in range(its):   # path is its?\n",
    "            SCC_values[tm, path] = SCC_func(hists[tm,:,path])\n",
    "            SCC1_values[tm, path] = SCC1_func(hists[tm,:,path])\n",
    "            SCC2_base_values[tm, path] = SCC2_base_func(hists[tm,:,path]) \n",
    "            SCC2_tilt_values[tm, path] = SCC2_tilt_func(hists[tm,:,path])\n",
    "            SCC2_V_d_baseline_values[tm, path] = SCC2_V_d_baseline_func(hists[tm,:,path])\n",
    "            SCC2_V_d_tilt_values[tm, path] = SCC2_V_d_tilt_func(hists[tm,:,path])\n",
    "\n",
    "    SCC_total = np.mean(SCC_values,axis = 1)     # Total SCC, \n",
    "    SCC_private = np.mean(SCC1_values,axis = 1)  # Private, resource scarcity\n",
    "    SCC2_FK_base = np.mean(SCC2_base_values,axis = 1)  # ecc_bar, expectation part\n",
    "    SCC2_FK_tilt = np.mean(SCC2_tilt_values,axis = 1)\n",
    "    SCC2_V_d_baseline = np.mean(SCC2_V_d_baseline_values,axis = 1) # ecc_bar first part; instantaneous contribution\n",
    "    SCC2_V_d_tilt = np.mean(SCC2_V_d_tilt_values,axis = 1)\n",
    "\n",
    "    SCCs = {}\n",
    "    SCCs['SCC'] = SCC_total\n",
    "    SCCs['SCC1'] = SCC_private\n",
    "    SCCs['SCC2'] = SCC2_FK_base + SCC2_V_d_baseline\n",
    "    SCCs['SCC3'] = SCC2_V_d_tilt - SCC2_V_d_baseline + SCC2_FK_tilt - SCC2_FK_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCCs['SCC3'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REs = {}\n",
    "Dists = {}\n",
    "a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)\n",
    "b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)\n",
    "a_10std = Œ≤ùòß - 10 * np.sqrt(œÉ·µ¶)\n",
    "b_10std = Œ≤ùòß + 10 * np.sqrt(œÉ·µ¶)\n",
    "\n",
    "RE_func_r = GridInterp(gridpoints, RE, method)\n",
    "def RE_func(x):\n",
    "    return RE_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "e_func_r = GridInterp(gridpoints, e, method)\n",
    "def e_func(x):\n",
    "    return e_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "pi_tilde_1_func_r = GridInterp(gridpoints, œÄÃÉ1, method)\n",
    "def pi_tilde_1_func(x):\n",
    "    return pi_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "lambda_tilde_1_func_r = GridInterp(gridpoints, ŒªÃÉ1, method)\n",
    "def lambda_tilde_1_func(x):\n",
    "    return lambda_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "beta_tilde_1_r = GridInterp(gridpoints, Œ≤ÃÉ1, method)\n",
    "def beta_tilde_1_func(x):\n",
    "    return beta_tilde_1_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "RE_plot = np.zeros(pers)\n",
    "weight_plot = np.zeros(pers)\n",
    "beta_f_space = np.linspace(a_10std,b_10std,200)\n",
    "\n",
    "#Relative Entropy\n",
    "\n",
    "if damageSpec == 'low':\n",
    "    nordhaus_mean = np.zeros(pers)\n",
    "    nordhaus_std = np.zeros(pers)\n",
    "\n",
    "    for tm in range(pers):\n",
    "        RE_plot[tm] = RE_func(hists[tm,:,0])\n",
    "        weight_plot[tm] = pi_tilde_1_func(hists[tm,:,0])\n",
    "        nordhaus_mean[tm] = beta_tilde_1_func(hists[tm,:,0])\n",
    "        nordhaus_std[tm] = 1 / np.sqrt(lambda_tilde_1_func(hists[tm,:,0]))\n",
    "\n",
    "    REs['RE'] = RE_plot\n",
    "    REs['Weights'] = weight_plot\n",
    "    REs['Shifted Mean'] = nordhaus_mean\n",
    "    REs['Shifted Std'] = nordhaus_std\n",
    "\n",
    "else:\n",
    "    for tm in range(pers):\n",
    "        RE_plot[tm] = RE_func(hists[tm,:,0])\n",
    "        weight_plot[tm] = pi_tilde_1_func(hists[tm,:,0])\n",
    "\n",
    "    REs['RE'] = RE_plot\n",
    "    REs['Weights'] = weight_plot\n",
    "\n",
    "\n",
    "original_dist = norm.pdf(beta_f_space, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "Dists['Original'] = original_dist\n",
    "    \n",
    "# probabilities (R,K,F)\n",
    "\n",
    "for tm in [1,100,200,300,400]:\n",
    "    R0 = hists[tm-1,0,0]\n",
    "    K0 = hists[tm-1,1,0]\n",
    "    F0 = hists[tm-1,2,0]\n",
    "\n",
    "    # Weitzman\n",
    "    def scale_2_fnc_prob(x):\n",
    "        return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 *  F0 + Œ≥2_plus * x * (x * F0 - FÃÑ) ** (power - 1) * ((x * F0 - FÃÑ) >= 0)) * R0 * e_func([R0, K0, F0])) * norm.pdf(x, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    scale_2_prob = quad_int(scale_2_fnc_prob, a, b, n, 'legendre')\n",
    "\n",
    "    q2_tilde_fnc_prob = np.exp(-1 / Œæp * xi_d * (Œ≥1 * beta_f_space + Œ≥2 * beta_f_space ** 2 * F0 + Œ≥2_plus * beta_f_space * (beta_f_space * F0 - FÃÑ) ** (power - 1) * ((beta_f_space * F0 - FÃÑ) >= 0)) * R0* e_func([R0, K0, F0])) / scale_2_prob * norm.pdf(beta_f_space, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    weitzman = q2_tilde_fnc_prob\n",
    "\n",
    "    # Nordhaus\n",
    "    mean_distort_nordhaus = beta_tilde_1_func([R0, K0, F0]) - Œ≤ùòß\n",
    "    lambda_tilde_nordhaus = lambda_tilde_1_func([R0, K0, F0])\n",
    "    nordhaus = norm.pdf(beta_f_space, mean_distort_nordhaus + Œ≤ùòß, 1 / np.sqrt(lambda_tilde_nordhaus))\n",
    "\n",
    "    # weights\n",
    "    Dists_weight = pi_tilde_1_func([R0, K0, F0])\n",
    "    if damageSpec == 'High':\n",
    "        Dists['Weitzman_year' + str(int((tm) / 4))] = weitzman\n",
    "    elif damageSpec == 'Low':\n",
    "        Dists['Nordhaus_year' + str(int((tm) / 4))] = nordhaus\n",
    "    elif damageSpec == 'Weighted':\n",
    "        Dists['Weitzman_year' + str(int((tm) / 4))] = weitzman\n",
    "        Dists['Nordhaus_year' + str(int((tm) / 4))] = nordhaus\n",
    "        Dists['Weighted_year' + str(int((tm) / 4))] = nordhaus * Dists_weight + weitzman * (1 - Dists_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as smart guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ = loadmat(r'C:\\Users\\jiamingwang\\Dropbox\\share with John\\Final Code/Growth_JY_Final/SCC_mat_Cumu_base_GrowthNoAmb_1e9.mat')\n",
    "base_guess = base_['v0']\n",
    "worst_ = loadmat(r'C:\\Users\\jiamingwang\\Dropbox\\share with John\\Final Code\\Growth_JY_Final/SCC_mat_Cumu_worst_GrowthNoAmb_1e9.mat')\n",
    "worst_guess = worst_['v0']\n",
    "check = loadmat(r'C:\\Users\\jiamingwang\\Dropbox\\share with John\\Final Code/Growth_JY_Final/HJB_Growth_Neutral')\n",
    "\n",
    "v0_guess = check['out_comp']\n",
    "q_guess = check['q']\n",
    "e_guess = check['e_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_guess = {}\n",
    "# prelin_results = {}\n",
    "key = 'GrowthNeutral'\n",
    "smart_guess = {}\n",
    "smart_guess['v0'] = v0_guess\n",
    "smart_guess['q'] = q_guess\n",
    "smart_guess['e'] = e_guess\n",
    "smart_guess['base'] = base_guess\n",
    "smart_guess['worst'] = worst_guess\n",
    "with open('./data/{}.pickle'.format(key + 'guess'), \"wb\") as file_:\n",
    "    pickle.dump(smart_guess, file_, -1)\n",
    "# smart_guess['WeightedNeutral'] = {}\n",
    "# smart_guess['WeightedNeutral']['v0'] = v0_guess\n",
    "# smart_guess['WeightedNeutral']['q'] = q_guess\n",
    "# smart_guess['WeightedNeutral']['e'] = e_guess\n",
    "# smart_guess['WeightedNeutral']['base'] = base_guess\n",
    "# smart_guess['WeightedNeutral']['worst'] = worst_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_guess.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/smartguesses.pickle', \"wb\") as file_:\n",
    "    pickle.dump(smart_guess, file_, -1)\n",
    "    \n",
    "# m1 = pickle.load(open('./data/comppref.pickle', \"rb\", -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open('./data/smartguesses.pickle', \"rb\", -1))['WeightedAverse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densityPlot(key = 'Weighted'):\n",
    "    years = [50, 75, 100]\n",
    "\n",
    "    titles = [\"Year {}\".format(year) for year in years]\n",
    "\n",
    "    fig = make_subplots(1, len(years), print_grid = False, subplot_titles = titles)\n",
    "\n",
    "    dom =beta_f_space\n",
    "    inds = ((dom>=0) & (dom<=5e-3))\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # data = loadmat(\"{}/50-50 weight/Dist_{}yr.mat\".format(quad_rule, year))\n",
    "        data = Dists\n",
    "        if key == 'Weighted': \n",
    "            if i == 0:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = True, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Nordhaus_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'Low Damage Function', line = dict(color = 'red', dash='dashdot', width = 3), showlegend = True, legendgroup = 'Low Damage Function')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Weitzman_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'High Damage Function', line = dict(color = 'green', dash='dash', width = 3), showlegend = True, legendgroup = 'High Damage Function')\n",
    "            else:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = False, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Nordhaus_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'Low Damage Function', line = dict(color = 'red', dash='dashdot', width = 3), showlegend = False, legendgroup = 'Low Damage Function')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Weitzman_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'High Damage Function', line = dict(color = 'green', dash='dash', width = 3), showlegend = False, legendgroup = 'High Damage Function')\n",
    "\n",
    "        elif key == 'High':\n",
    "            if i == 0:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = True, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Weitzman_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'High Damage Function', line = dict(color = 'green', dash='dash', width = 3), showlegend = True, legendgroup = 'High Damage Function')\n",
    "            else:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = False, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Weitzman_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'High Damage Function', line = dict(color = 'green', dash='dash', width = 3), showlegend = False, legendgroup = 'High Damage Function')\n",
    "\n",
    "\n",
    "        elif key == 'Low':\n",
    "            if i == 0:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = True, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Nordhaus_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'Low Damage Function', line = dict(color = 'red', dash='dashdot', width = 3), showlegend = True, legendgroup = 'Low Damage Function')\n",
    "            else:\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Original'][inds], row = 1, col = i + 1,\n",
    "                    name = 'Original Distribution', line = dict(color = '#1f77b4', width = 3), showlegend = False, legendgroup = 'Original Distribution')\n",
    "                fig.add_scatter(x = dom[inds] * 1000, y = data['Nordhaus_year' + str(year)][inds], row = 1, col = i + 1,\n",
    "                    name = 'Low Damage Function', line = dict(color = 'red', dash='dashdot', width = 3), showlegend = False, legendgroup = 'Low Damage Function')\n",
    "\n",
    "    fig['layout'].update(title = key + \" Damage Specification\", showlegend = True, titlefont = dict(size = 20), height = 400)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "\n",
    "        fig['layout']['yaxis{}'.format(i+1)].update(showgrid = False)\n",
    "        fig['layout']['xaxis{}'.format(i+1)].update(showgrid = False)\n",
    "\n",
    "    fig['layout']['yaxis1'].update(title=go.layout.yaxis.Title(\n",
    "                                    text=\"Probability Density\", font=dict(size=16)))\n",
    "    fig['layout']['xaxis2'].update(title=go.layout.xaxis.Title(\n",
    "                                    text=\"Climate Sensitivity\", font=dict(size=16)), showgrid = False)\n",
    "\n",
    "    fig = go.FigureWidget(fig)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityPlot(damageSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCCDecomposePlot(key = 'Weighted'):\n",
    "\n",
    "    if key == 'Low':\n",
    "\n",
    "        data = SCCs\n",
    "        x1, y1, x2, y2, x3, y3 = 60, 195, 93, 330, 96, 100\n",
    "\n",
    "    elif key == 'Weighted':\n",
    "\n",
    "        data = SCCs\n",
    "        x1, y1, x2, y2, x3, y3 = 60, 320, 80, 315, 90, 350\n",
    "\n",
    "    elif key == 'High':\n",
    "\n",
    "        data = SCCs\n",
    "        x1, y1, x2, y2, x3, y3 = 60, 340, 93, 495, 96, 430\n",
    "\n",
    "\n",
    "    total_SCC = np.array(data['SCC'])\n",
    "    external_SCC = np.array(data['SCC2'])\n",
    "    uncertainty_SCC = np.array(data['SCC3'])\n",
    "    private_SCC = np.array(data['SCC1'])\n",
    "    x = np.linspace(0,100,400)\n",
    "\n",
    "    total = go.Scatter(x = x, y = total_SCC,\n",
    "                   name = 'Total', line = dict(color = '#1f77b4', dash = 'solid', width = 3),\\\n",
    "                       showlegend = False)\n",
    "    external = go.Scatter(x = x, y = external_SCC,\n",
    "                   name = 'Ambiguity', line = dict(color = 'red', dash = 'dot', width = 3),\\\n",
    "                          showlegend = False)\n",
    "    uncertainty = go.Scatter(x = x, y = uncertainty_SCC,\n",
    "                   name = 'No Ambiguity', line = dict(color = 'green', dash = 'dashdot', width = 3),\\\n",
    "                             showlegend = False)\n",
    "    private = go.Scatter(x = x, y = private_SCC,\n",
    "                   name = 'Private', line = dict(color = 'black', width = 3),\\\n",
    "                         showlegend = False)\n",
    "\n",
    "    annotations=[dict(x=x1, y=y1, text=\"Total\", textangle=0, ax=-100,\n",
    "                ay=-75, font=dict(color=\"black\", size=12), arrowcolor=\"black\",\n",
    "                arrowsize=3, arrowwidth=1, arrowhead=1),\n",
    "\n",
    "                dict(x=x2, y=y2, text=\"Ambiguity\", textangle=0, ax=-100,\n",
    "                ay=0, font=dict(color=\"black\", size=12), arrowcolor=\"black\",\n",
    "                arrowsize=3, arrowwidth=1, arrowhead=1),\n",
    "\n",
    "                dict(x=x3, y=y3, text=\"No Ambiguity\", textangle=0, ax=-80,\n",
    "                ay=80, font=dict(color=\"black\", size=12), arrowcolor=\"black\",\n",
    "                arrowsize=3, arrowwidth=1, arrowhead=1)]\n",
    "\n",
    "    layout = dict(title = 'Social Cost of Carbon, {} Damage Specification'.format(key),\n",
    "                  titlefont = dict(size = 20),\n",
    "                  xaxis = go.layout.XAxis(title=go.layout.xaxis.Title(\n",
    "                                    text='Years', font=dict(size=16)),\n",
    "                                         tickfont=dict(size=12), showgrid = False),\n",
    "                  yaxis = go.layout.YAxis(title=go.layout.yaxis.Title(\n",
    "                                    text='Dollars per Ton of Carbon', font=dict(size=16)),\n",
    "                                         tickfont=dict(size=12), showgrid = False), \n",
    "                  annotations=annotations\n",
    "                  )\n",
    "\n",
    "    fig = dict(data = [total, external, uncertainty], layout = layout)\n",
    "    iplot(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCCDecomposePlot(damageSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionPlot(damageSpec, Œæ):\n",
    "\n",
    "    colors = {'High': 'red', 'Low': 'green', 'Weighted': '#1f77b4'}\n",
    "    lines = {'Averse': 'solid', \"Neutral\": 'dashdot'}\n",
    "\n",
    "    # damageSpecs = ['High', 'Low', 'Weighted']\n",
    "    # aversionSpecs = ['Averse', 'Neutral']\n",
    "    # colors = ['green', '#1f77b4', 'red']\n",
    "    # lines = ['solid', 'dashdot'] \n",
    "\n",
    "    x = np.linspace(0, 100, 400)\n",
    "    data = []\n",
    "\n",
    "    data.append(go.Scatter(x = x, y = e_hists[:,0], name = damageSpec +  ' Damage w/ Œæ= {}'.format(Œæ),\n",
    "        line = dict(width = 2, dash = 'solid', color = colors[damageSpec]), showlegend = True))\n",
    "\n",
    "    layout = dict(title = 'Emissions Plot with {} Damage Setting, Œæ = {}'.format(damageSpec, Œæ),\n",
    "      titlefont = dict(size = 20),\n",
    "      xaxis = go.layout.XAxis(title=go.layout.xaxis.Title(\n",
    "                        text='Years', font=dict(size=16)),\n",
    "                             tickfont=dict(size=12), showgrid = False, showline = True),\n",
    "      yaxis = go.layout.YAxis(title=go.layout.yaxis.Title(\n",
    "                        text='Gigatons of Carbon', font=dict(size=16)),\n",
    "                             tickfont=dict(size=12), showgrid = False),\n",
    "      legend = dict(orientation = 'h', y = 1.15)\n",
    "      )\n",
    "\n",
    "    fig = dict(data = data, layout = layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissionPlot(damageSpec, Œæp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
